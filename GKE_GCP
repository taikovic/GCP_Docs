--------------------------------------------- Containers ---------------------------------------
 in GCP: activate the Google Container Registry API
 create new project
 activate: Cloud Shell
 gcloud auth  login // for login;
 create a Dockerfile
----------------------------------
 *) create a container:
  docker build -t myapp (tag the container we built: myapp)
 docker images (check container  is there)
 docker run -d -p (mapping port ex: 8888:8888) myapp (running new containers)
 docker ps (check the img is running with name)
 docker exec -it name_container /bin/bash (enter to the container)
 docker stop|start name_container (stopping|starting existing containers)
 docker rm name_container (remove the container not the image)
 --------------------------------
 docker tag myapp:latest gcr.io/myproject/myapp:official_name (tag first: prepare my image to push to google cloud registry)
 docker images (see the image name)
 docker push gcr.io/myproject/myapp:official_name (check in GCR)

 ------------------------------------------------ Google Kubernetes: Definitions  ---------------------------------
*) Cluster:
- Master: 1+, CONTROL PLANE, make decisions (scheduling) running on VMs
          APIServer: front communication (our day to day front w/  GKE)
          etcd: DB (key,value store) for configuration
          Scheduler: schedule loads
          Cloud controller mgr: to work for google cloud (networking, LB)
          Kube controller mgr: manager controller like nodes
- Node: runtime env, rcs
        Kubelet: Agent with Master
        kube-proxy: manager network
        container-runtime: to work

*) Building K cluster:
- Manually: provision VMs (for master and nodes)
           Install Kubernetes, create network overlay, set up CA
- Easy: GKE

*) Kubectl: manage, run the cluster via APIServer, by defining objects in Kubernetes
            EVERYTHING IN KUBERNETES IS OBJECT
  in GCR: search for the container to deploy: deploy to GKE, in worloads dont forget to expose port.
          the LB will expose the service and get external IP
------------------------------------------------ Google Kubernetes: Managing -----------------------------------------
*) to use Kubectl (CLI) first:
  gcloud container clusters get-credentials name_cluster --zone=cluster_zone
*) check nodes:
  kubectl get nodes (nodes: vms)
  kubectl get pods (apps running)
  kubectl get services
*) make a deployment:
  kubectl create deployment nginx --image nginx  (deployment.apps/nginx created)
  kubectl get pods (check nginx)
*) expose port, IPadress:
  kubectl expose deployment nginx --port=80 --type=LoadBalancer
  kubectl get services
-------------------------------
*) Pods: smallest  unit to deploy (object)
        host containers (1 to 3) sharing same ip address, volumes...
        1 application: monolith and webserver, 1 app + db, 1app + tool to process output
        multiple pods can be deployed on 1 kubernete Node
*) create Pod:
        with Kubectl
        with YAML config file (declaratively, kind: Pod) (ca demande sur GCP: gcloud auth login)
        $ kubectl apply -f mypod.yaml (create the pod based on config file yaml)
        $ kubectl describe deployment mypod (check details abt deployment)
        tip:pour checker un port ouvert (80), on utilise "web preview" du cloud shell.on tunnel-up le port 80 au port interne 8080
          $ kubeclt port-forward nginx 8080:80
        $kubectl delete -f mypod.yaml (delete objects created by yaml file)
*) Multi container in 1 Pod:
        multiple container can share the same Pod. sharing same env and volumes
        $kubectl apply -f multi.yaml
        to get into 1 container:
        $kubectl exec -it multi -c ftp-container -- /bin/bash (-c: only when multi-containers)
-----------------------------------
*) ReplicaSet:
      replicas of Pod objects, we can specify number of pods, not recommended to create RS on their own
      fait en sorte d avoir tjrs le nombre defini de pods quoiqu'il en soit la situation
      on fait mieux:
*)Deployment: gere bien les  RS(a definir ds yaml)
      object: managing pods and RS
      Desired state is enforce by Deployment controller
      Les pods changent  avec le maintien des RS sur les noeuds (cas de failure)
      coment alors controler l acces des Pods avec ce changement ? avec les services
*)Service: YAML config also could be set
      Exposes a SET OF PODS to the network : Assign fixed IP to Pod Replicas
      3 main services:
        ClusterIP(by default when Type not specified): accessed inside the cluster
        NodePort,
        LoadBalancer: accessed outside the cluster: exposition a l externe
      configuration: with Selectors
      Labels are key-value  pairs  in object metadata (ex: app=nginx: chaque pod qui a app egal a nginx, fera partie du service)
      $kubectl apply -f myapp-service.yaml (run the service)
      $kubectl get services (to check: if type: LB check external-ip OK)

Exo Availability: Z!!! Tips:
- simulating one pod down: $kubectl delete pod myapp-deployment-1_of_3pods
- voir les noeuds (VMs) et pods associes:
  $kubectl get pods -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name
- corrompons un noeud: avec Taint!! et ensuite enlevons de pod qui run dessus;
  $kubectl taint nodes (1_des_noeuds) key=value:NoSchedule (ajouter une metadata au noeud pour dire au scheduler de ne pas l utiliser)
  $kubectl delete pod (pod running on node tainted)
  $kubectl get pods -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name
  ==> Ceci va montrer qu un nouveau Pod est cree (le job de ReplicaSet de s assurer qu on le bon nombre), sur un noeud existant.
  refaire UP le noeud:
  $kubectl taint nodes (1_des_noeuds) key:NoSchedule-
Exo Rollback:
  When updating an appcode and apply new deployment(we dont WANT or by MISTAKE): ==> new replicSet of pods is created.
  $kubectl apply -f myapp-deployment.yaml --record
  $kubectl rollout status deployment.v1.apps/myapp-deployment
  to roll back an app to defined versionv1:
  $kubectl rollout undo deployment.v1.apps/myapp-deployment
  check de RS: $kubectl get pods (new pods)

  --------------------------------------------------------------------Troubleshooting, Monitoring Logging, Debuggin ----------------------------------------------

 *) Monitoring and Logging:
 dynamically creates deployments:
 $kubectl create deployment nginx --image=nginx
 $kubectl get pods
 $kubectl describe pod nginx (config and events)
 $kubectl expose deployment nginx --port=80 --type=LoadBalancer

*) Stackdriver:ALWAYS logs with POD
 get the Logs:
 $kubectl logs pod/nginx-86c57db685-ds8dr or for multiple pods:
 $kubectl logs --selector run=nginx
 $kubectl logs pod/myapp-deployment-id (-c container_name)

 *) Debugging failed DeploymentSpec
  faire un failure:
  - mettre plusieurs containers  dans un pod
  $kubectl scale --replicas=10 deployment nginx
  faire les checks;
  - mauvais pods:
  $kubectl run badluck1 --image=gcr.io/my-gke-project-300518/myapp:v42
  le pod badluck1 cree mais ErrImagePull comme status (v42 doesn t exist)
  $kubectl delete pod badluck1
  $ kubectl delete --all pods (deployments, services)
  - code applicatif pas  bon:
  $kubectl run badluck2 --image=gcr.io/my-gke-project-300518/myapp:broken
  $kubectl describe pod badluck2 (for confirming container error)
  $$ kubectl logs pod/badluck2
---------------------------------------------------------------------------Deploying Applications -------------------------------------------------------------------
*) Pod reliability with health checks:(liveness probes: mijassate)
- config Probe, performed by Handler (ExecAcion, TCPSocketAction, HTTPGetAction (200ok))
- use: initialDelaySeconds, periodSeconds, timeoutSeconds; success/failure Threshold
- pod states: pending, running, successing (action ok)
- use: livenessProbe or readinessProbe(if takes time to be up: importing data eg) OR BOTH
*) !!!!CATCHER DES PROBLEMES AVEC DES PODS QUI TOURNENT AVEC TRAFFIC EN PROD: LIVENESSPROBE
  - en image (docker) access (Endpoints: /... web ) via HTTPGet, every 3 seconds (initialDelaySeconds,periodSeconds) to test health of PODS
   (to configure in deployment yaml file)
  - qd les pods redemarrent  infiniment (a cause de pb de code: connection to DB impossible par ex), les pods se mettent dans le status,
    CrashLoopBackOff...
  - Kubernets se rend compte que le restart ne fixe pas le pb. et rebelotte, on redemarre
  - le restart peut aider, si une application a besoin de memoire (out of memory) mais pas de pb fondamental de code;

*) CATCHER DES PROBLEMES DE PODS AVANT LE TRAFFIC EN PROD: readinessProbe
  - si les pods ne sont pas en succes (avec le check de readinessProbe), les pods ne seront pas ready.
  donc ne seront pas alloues pour servir le traffic (probabelement externe du LB ou autres)
*) TJRS: LES ANCIENS PODS, ROUTENT LE TRAFFIC AUX NOUVEAUX:
------------------------------------------------------------------------------- Accessing External Services -----------------------------------------------------------
2 ways:
*) Service Endpoints:
  - create service with no Selector : K8S looks for an endpoint w/ the same NAME;
  - endpoints: External Services mapping to Service objects (internal DNS to Access services)
  - Use type "ExternalName" to map to externalFQDNs(no endpoints required)
*) SideCars: another container to access external service (to connect)
    app can access to sidecar container via localhost/127.0.0.1
---------------------------------------------------------------------
exo: as a sidecar:with myapp:
  - add a container of mysql DB instance;
  - myapp should have a service account to access mysql db
  - create a service Acct: gcloud iam service-accounts create cloudsqlproxy
  - assign permissions:
  gcloud projects add-iam-policy-binding my-gke-project-300518
  --member serviceAccount:cloudsqlproxy@my-gke-project-300518.iam.gserviceaccount.com --role roles/cloudsql.client
  - add key for service account cloudsqlproxy as json :
  gcloud iam service-accounts keys create ./sqlproxy.json --iam-account cloudsqlproxy@my-gke-project-300518.iam.gserviceaccount.com
   - get the DB "connection name" and go to deployment.yaml to add a container (as sidecar to myapp)
   -------------------------------------------------------------------------------- Volumes and Persistent Storage ----------------------------------------------------------
  *)  Container is ephemeral
  - Volume independant object from app container
  - to Access Volume files : mount directory in a container
  *) Volume Types:
      -- EmptyDir: shared by xxx container in the same Pod: deleted when pod removed from a node;
      -- gcePersistentDisk: must be created beforehand, readonly by consumers, only unmounted after removing POD
      -- PersistentVolumeClaim: claim for gke to request a PersistentVolume into a Pod. no other claim can reserve the same PV.
  - PersistentVolume:
      - manually: using existing GCP persistent disk w/ filesystem
  - Access Modes:
      - ReadWriteOne: a  single node can mount the volume read/write
      - ReadOnlyMany: any node can mount the volume ReadOnly
      - ReadWriteMany: any node can mount the volume Read/Write (not supported by gCP persistend disk)
  *) Constraints:
      - keep in mind (pods are distributed across different nodes) and deployments designed for stateless apps
      - all replicas will share the same PersistentVolumeClaim: volume mode should be ReadOnlyMany (not readwrite only one single Pods in RS)
      - deployment strategy may cause volume Deadlocks
  *) Alternatives:
      - GCS: object storage,
      - managed Databases
      - Cloud Filestore
Exo:
  - create persistentVolumeClaim.yaml file (kubectl apply -f dynamically)
  - check: kubectl get pvc
  - To create a secret object (to store usr/key pair):kubectl create secret generic mysql --from-literal=password=MYSECRETPASS
  !! instead of deleting a pod with the full name: mysql-7fb84d4db9-wvsbh, we can delete pod with labels key/value:
          $ kubectl delete pod -l app=mysql
          $ kubectl delete -f wordpress-service.yaml (delete the service w/file)
------------------------------------------------------------------------------------------ Special Volume Types ---------------------------------------

*) Secrets:
  - Designed to Store Sensitive Data,ex: credentials
  - allows insert sensitive information at container runtime (not in the image)
  - consumed: soit des variables d'env ou mount in FS container w/ volumes
  - pas crypte par default, encode ds K8s (cluster)
*) ConfigMaps:
  - Decouple  config from image content; used in complex deployments
    - Updating some settings or env variables when containers run
    - having multiple images w/ some minor changes
  - create configMaps from: config files, directory of config files or key-value pairs
  - 2 methods:
      x) containers run => insert values from 'configMaps' as Env variables (inside containers) for the system to access
          use env (choose variables to use w/configKeyRef) or the whole configmaps w/ envFrom (configMapRef) and the name of configMap
      x) Mount 'configMaps' data as Volume, container will then access config files without changing underlying image
          on fait mount du configmap (defini ds un volumes) dans un FS (ds volumeMounts, dans mountPath)
  - configmaps:  defined in Yaml,

------------------------------------------------------------------------------------------- Deployment patterns ------------------------------------------
1) - RollingUpdate Deployment: default deployment for stateless  deployment: new pods replacing  old pods; with max unavailable pods, 
2) - Canary pattern:     multiple deployment w/ One single service, so small subset of traffic but in Production with Env Canary
                        Service selector should take all app (No specific Env)
3) - Blue/Green pattern: maintain 2 versions of app.With service selector, switch traffic from v1 to v2
                        all traffic immediately sent to new deployment. In case of error, switch back to old version.
                        in Service selector, add app version.

Exo:
    docker build -t + docker tag ... + docker push gcr.io/....
      => with google CLI: gcloud builds submit --tag gcr.io/my-project/myapp:v1
--------------------------------------------------------------------------------------- Autoscaling ----------------------------------------------------

*) Autoscaling Pods horizontaly:
 - HorizontalPodAutoscaler: object => yaml file for this object Kind, W/ scaleTargetRef in spec as the application
 - autoscal the number of Pod replicas
 - cpu and ram threshold in config, and min/max number of pods (minReplicas, maxReplicas, targetCPUUtilizationPercentage)
 - custom metrics: applications, stackdriver metrics

*) Autoscaling Pods vertically: 
 - increasing rsrc: CPU and RAM 
 - VerticalPodAutoscaler: object as kind in yaml file
 - updatePolicy: updateMode= auto => pods will be restarted
 - for STATEFUL Applications

*) Autoscaling cluster: 
- Scale cluster Nodes: Node-pool autoscaling cluster in GKE.
- group VMs in Node-pools; ram-intensive consumption get their node-pool
---------------------------------------------------------------------------- Advanced: Helm -----------------------------------------------------------

*) Helm: 
        - Pks kukbernetes (obj manifests and configs) in -> Helm's Chart 
        - maintains the lifecycle of deployment in K8s (named release)
        - 2 components: helm (CLI) & Tiller (server side) 
        - public repos for helm charts
    Usage: 
       - install helm -> initialize helm for cluster -> install helm chart (like: helm install stable/wordpress) -> helm applies manifests (in template) and specific config values
        (values.yaml file) or default ones using also metadata (in chart.yaml and requirements.yaml)
Exo: 
       - $kubectl apply -f tiller-serviceaccount.yaml (in my local directorie: exists) => to Set-UP Tiller in the new cluster;  
       - $helm init --service-account AcctName => install helm to the Kubernetes cluster; 
       - $kubectl get deploy,svc tiller-deploy -n kube-system : chek tiller is up and running on cluster (== kubectl get deployment tiller-deploy -n xxxx)
       - $helm install stable/bookstack (install the charts bookstack)
       - $helm status name_release (check status w fill infos)
       - $kubectl get pv (persistent volumes)
   To change a value in release, we should upgrade it, exemple: 
       - $helm upgrade --set service.type='LoadBalancer' dozing-robin stable/bookstack /to expose laodbalancer to web service;
       - $helm list (list our releases on the cluster ) 
       - $helm delete release-name; delete all pods/deployments/services/statefulsets/ relative to the release
   Install a Chart with a Local values.yaml file: 
       - $helm install -f values.yaml stable/bookstack --name taikbook
--------------------------------------------------------------------------- Ingress Control ---------------------------------------------------------------
*) Ingress: 
      - Expose traffic in Customized way than the LoadBalancer; 
      - for each service -> we configure an Ingress Object (defines How to route traffic): access the service from Outside Cluster 
      - Designed for HTTP/s; securing w/ SSL, Routing to different Backend Systems based on: host names or URL paths (name/path -based routing)
      - Define Ingress Resource ==> Ingress Controller: routes Traffic based on resources; 
      - NGINX Ingress controller deployment is popular choice; 
      - public ----> LoadBalancer(GC) -----> Ingress Controller ---> [ Service1,2,...,n  --------> Pods1,2,...,n ]_GKCluster 
      - Ingress Controller checks Ingress resources to route traffic to services 
      - resolve with curl: 
        $curl --resolve myapp.example.com:80:35.238.32.151 http://myapp.example.com : qd on cherhe la page, elle a ete mappe avec le publicIP de l Ingress ctrler
         qui va router le traffic au bon service;
-------------------------------------------------------------------------------------------------Cluster high Availability ---------------------------------------------------------
*) workloads highly available: Create Regional GKE Clusters: 
*) Storage highly available: Regional Persistent Storage; use nodeAffinity and nodeSelectorTerms
*) maximum availability: run application accross  Multi-cluster Ingress (laodbalancer) across multiple GKE clusters
   - use of kubemci;
   exo:
    - $KUBECONFIG=clusters.yaml gcloud container clusters create --zone=us-east4-a cluster-1 (create 1st cluster)
    - $KUBECONFIG=clusters.yaml gcloud container clusters create --zone=europe-west1-c cluster-2 (create 2nd one)
    Ceci cree un fichier clusters.yaml avec  ts les details des clusters avec leurs contextes
    - $ kubectl --kubeconfig=clusters.yaml --context=project-xxxx_us-east4-a_cluster-1 apply -f manifests/
             - chercher le context ds clusters.yaml et appliquer tous les manifests (directory btw)
             - faire le meme traitement pour tous les contexts; ou bien une boucle do: (se placer ds la bonne director) 
             
    - to Check pods or other stuff, we should always use KUBECTL WITH KUBECONFIG LOCAL FILE AND SPECIFY THE CONTEXT (CLUSTER)
    - $kubectl --kubeconfig=clusters.yaml config get-contexts => for all contexts 
    - $kubectl --kubeconfig=clusters.yaml config current-context => to check current context 
    - $kubectl --kubeconfig=clusters.yaml config use-context xxxx_2nd_cluster => to change for a new cluster 
   Mgt w/ kubemci:  
    - wget https://storage.googleapis.com/kubemci-release/latest/bin/linux/amd64/kubemci; install and make it executable;
    - kubemci is going to need a static public IP address; faut reserver une address: 
    - gcloud compute addresses create --global kubemci-ip ;
    - gcloud compute addresses list; 
    - Creer le Ingress object: yaml  file; (inclure le kubemci-ip)
    - $./kubemci create zone-printer --ingress=manifests/ingress.yaml --kubeconfig=clusters.yaml
    KUBEMCI: 
      - Creates Ingress in each cluster 
      - configure a LoadBalancer 
      - creates whats needed for a loadbalancer;
       (Rules, instance groups for cluster, backend services, url maps, target proxies, forwarding rules and health checks)
    check mci: 
      -$ ./kubemci get-status zone-printer; try to connect with the static ip address;
    
    


























      --
