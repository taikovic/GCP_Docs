---------------------------------------------------------------------------- Advanced: Helm -----------------------------------------------------------

*) Helm: 
        - Pks kukbernetes (obj manifests and configs) in -> Helm's Chart 
        - maintains the lifecycle of deployment in K8s (named release)
        - 2 components: helm (CLI) & Tiller (server side) 
        - public repos for helm charts
    Usage: 
       - install helm -> initialize helm for cluster -> install helm chart (like: helm install stable/wordpress) -> helm applies manifests (in template) and specific config values
        (values.yaml file) or default ones using also metadata (in chart.yaml and requirements.yaml)
Exo: 
       - $kubectl apply -f tiller-serviceaccount.yaml (in my local directorie: exists) => to Set-UP Tiller in the new cluster;  
       - $helm init --service-account AcctName => install helm to the Kubernetes cluster; 
       - $kubectl get deploy,svc tiller-deploy -n kube-system : chek tiller is up and running on cluster (== kubectl get deployment tiller-deploy -n xxxx)
       - $helm install stable/bookstack (install the charts bookstack)
       - $helm status name_release (check status w fill infos)
       - $kubectl get pv (persistent volumes)
   To change a value in release, we should upgrade it, exemple: 
       - $helm upgrade --set service.type='LoadBalancer' dozing-robin stable/bookstack /to expose laodbalancer to web service;
       - $helm list (list our releases on the cluster ) 
       - $helm delete release-name; delete all pods/deployments/services/statefulsets/ relative to the release
   Install a Chart with a Local values.yaml file: 
       - $helm install -f values.yaml stable/bookstack --name taikbook
*) Helm will use its public git repo by default, but can be configured to use other public or private chart repos, or deploy charts from a local filesystem.

--------------------------------------------------------------------------- Ingress Control ---------------------------------------------------------------
*) Ingress: 
      - Expose traffic in Customized way than the LoadBalancer; 
      - for each service -> we configure an Ingress Object (defines How to route traffic): access the service from Outside Cluster 
      - Designed for HTTP/s; securing w/ SSL, Routing to different Backend Systems based on: host names or URL paths (name/path -based routing)
      - Define Ingress Resource ==> Ingress Controller: routes Traffic based on resources; 
      - NGINX Ingress controller deployment is popular choice; 
      - public ----> LoadBalancer(GC) -----> Ingress Controller ---> [ Service1,2,...,n  --------> Pods1,2,...,n ]_GKCluster 
      - Ingress Controller checks Ingress resources to route traffic to services 
      - resolve with curl: 
        $curl --resolve myapp.example.com:80:35.238.32.151 http://myapp.example.com : qd on cherhe la page, elle a ete mappe avec le publicIP de l Ingress ctrler
         qui va router le traffic au bon service;
-------------------------------------------------------------------------------------------------Cluster high Availability ---------------------------------------------------------
*) workloads highly available: Create Regional GKE Clusters:
*) A regional cluster provides a single static endpoint for the entire cluster and spreads your cluster's Pods across multiple zones of a given region. 
   In the event of the loss of a zone, your Pods can be rescheduled and the control plane remains available. However, 
   you may still have constraints around the use and availability of persistent disks.
*) Storage highly available: Regional Persistent Storage; use nodeAffinity and nodeSelectorTerms
*) maximum availability: run application across  Multi-cluster Ingress (laodbalancer) across multiple GKE clusters
   - use of kubemci;
   exo:
    - $KUBECONFIG=clusters.yaml gcloud container clusters create --zone=us-east4-a cluster-1 (create 1st cluster)
    - $KUBECONFIG=clusters.yaml gcloud container clusters create --zone=europe-west1-c cluster-2 (create 2nd one)
    Ceci cree un fichier clusters.yaml avec  ts les details des clusters avec leurs contextes
    - $ kubectl --kubeconfig=clusters.yaml --context=project-xxxx_us-east4-a_cluster-1 apply -f manifests/
             - chercher le context ds clusters.yaml et appliquer tous les manifests (directory btw)
             - faire le meme traitement pour tous les contexts; ou bien une boucle do: (se placer ds la bonne director) 
             
    - to Check pods or other stuff, we should always use KUBECTL WITH KUBECONFIG LOCAL FILE AND SPECIFY THE CONTEXT (CLUSTER)
    - $kubectl --kubeconfig=clusters.yaml config get-contexts => for all contexts 
    - $kubectl --kubeconfig=clusters.yaml config current-context => to check current context 
    - $kubectl --kubeconfig=clusters.yaml config use-context xxxx_2nd_cluster => to change for a new cluster 
   Mgt w/ kubemci:  
    - wget https://storage.googleapis.com/kubemci-release/latest/bin/linux/amd64/kubemci; install and make it executable;
    - kubemci is going to need a static public IP address; faut reserver une address: 
    - gcloud compute addresses create --global kubemci-ip ;
    - gcloud compute addresses list; 
    - Creer le Ingress object: yaml  file; (inclure le kubemci-ip)
    - $./kubemci create zone-printer --ingress=manifests/ingress.yaml --kubeconfig=clusters.yaml
    KUBEMCI: 
      - Creates Ingress in each cluster 
      - configure a LoadBalancer 
      - creates whats needed for a loadbalancer;
       (Rules, instance groups for cluster, backend services, url maps, target proxies, forwarding rules and health checks)
    check mci: 
      -$ ./kubemci get-status zone-printer; try to connect with the static ip address;
 *) The kubectl cordon and drain options allow you to safely remove Pods from a node and prevent further Pods being scheduled there. 
 This can also be helpful when you need to perform maintenance on a node, like upgrading Kubernetes.
------------------------------------------------------------------------------------------ Cloud Native Security ------------------------------------------

* think about your security considerations in cloud : 4 C's 
        - Cloud: Think to project, VPC and IAM configuration, how they rely on any cloud resources running...
                  ex: cluster nodes could be accessed outside the project;
        - Cluster:Gke cluster; secure cluster: RBAC (role-based ac), namespaces and resources restrictions,
                                Pod security policies, network policies, workload identity
        - Container: trust, latest version w/ all necessary security updates
        - Code: secure your code

*) RBAC: 
        - regulating access to cluster resources;
        - roles defined at namespace OR Cluster Level; 
        - Grants a specific set of actions to specific API groups and resources
        - Role applies to namespaces, ClusterRole apply to clusters (could be on any namespaces)
*) Namespaces: 
        - Isolate resources for multiple teams and projects  on the same cluster => are virtual clusters
        - divide gKE cluster resources  with quotas;
        - a scope for resource names: object names have to be UNIQUE within namespace; 
        - 2 namespaces in GKE by default: default + kube-system
        - Pods run in kube-system namespace; 
        - define a namespace in yaml file; $kubectl -n kube-system get pods (-n: namespace)
        - Resource quotas attached to Namespaces can restrict the total amount of resource users of that Namespace may use. 
                Additionally, RBAC can be used to guarantee developers in each team only have access to their own Namespace.
*) Pod security policies: 
        - set of ctrls, conditions that Pod spec must meet in order to be scheduled; 
        - to enable pod security policy: 
                use of admission controller + creation of RBAC (role/clusterRole) w/permission to use policy
                role must be bind to user or service account; 
                without those actions, no pods will be scheduled; (podSecurityPolicy yaml)
*) network policy:
        - network access that Pods have; 
        - kubernetes object that defines network ingress and egress rules for pods (like firewall rules)
        - use selectors (to select pods) to restrict incoming/outgoing traffic
        - can Isolate traffic between namespaces 
        - reduce risk of application vulnerabilities affecting other cluster services
        - should be enabled when gke cluster is built (sinon recreation des pods)
        - networkPolicy yaml (ingress: incoming traffic)
        - in security context: we can add IPC_LOCK for memory (in OS) to not let pods accessing shared disk, to access files 
                swapped between memory and disk in linux (in case of sensitive data)
*) workload identity: 
        - secure gke workloads to consume gcp services; use service account;
----------------------------------------------------------------------------------------------- DaemonSets ------------------------------------------------------------

*) model: 
        - DaemonSets can be used to deploy ongoing background tasks that need to run on all or certain nodes in your cluster. 
                They can be useful for system daemons supporting storage, log collection and monitoring.
        - One Pod per Node model: no replicaSet here, no scaling;
        - each new node, will get new pod from DaemonSet;
        - Specific uses: storage daemon for each node, custom logs, metrics collection service, monitoring daemons,     
                          install custom drivers per node: GPU nodes install nvidia drivers (yaml file) 

----------------------------------------------------------------------------------------------- Stateful applications -----------------------------------------------------------

*) Stateless: 
        - pods can be added, removed, restarted at will;
        - individual pods have no data to persist, or concept of state; 
 *) stateful:
        - Pods have data to store and need persistent identity 
        - deployment and scaling of pods must be logically managed:  must be rules 
 To meet those requirement Kubernetes propose: 
 
 *) StatefulSets: (vs deployments) 
        - objects: that manages Pods based on a container spec, much like a Deployment
        - Maintains an IDENTITY for each deployed POD (unlike deployment) 
        - Guarantees the ordering and uniqueness of Pods:When Pods in a StatefulSet are being deleted, they are terminated and removed in reverse order.
        - Guarantees  Stable Network identity and persistent storage
        - Ordered, graceful deployment, scaling and updates
        - yaml manifest file; add: serviceName, 
*) update strategy: 
        - rolling updates delete and recreate pods in descending order 
        - scheduling waits for pods to be running and ready before proceeding
        - updates can be partitioned
        - OnDelete Strategy: manual updates can be enforced with the strategy
*) Persistent disks in StatefulSets are retained even if a Stateful Pod is removed, and must be manually deleted.
*) StatefulSet on a regional GKE cluster in the event of the loss of a GCP zone:
        -- GKE may attempt to reschedule the Stateful Pod in a zone where its persistent disk will not exist, 
                so the Pod will no longer have access to its PersistentVolume
        -- Regular GCP persistent disks are zonal resources, so cannot be mounted by nodes in other zones. 
                There are some complex workarounds involving regional persistent disks and node affinities however.

--------------------------------------------------------------------------------------------------- Finite tasks and init  Containers -------------------------------------------------------

*) Jobs and cronJobs: 
        - workload object that represents a finite task and manages it to completion;
        - a non-parallel job: starts a pod, when the pods completes successfully => job is complete
        - for a job: a pod spec, would involve a container and arguments to perform finite action; (not starting service)
        - for multiple pods: use (completions: n) 1 one job at once;
        - parallel jobs: have fixed completion count (how many jobs to run at once), running multiple pods to completion
                         - useful for running batches of work 
                         - parallelism is configurable, (use parallelism:n ) n jobs at once
        - Jobs can be scheduled using cronjobs: use (schedule: "   ", and jobTemplate for pods
 *) Init containers: 
        - in Pod Spec: part of containers array;
        - Execute BEFORE application or other containers in a pod;
        - run in order (w/other inint containers) and to completion
        - can separate start-up code from an application image;
        - can contain logic that delays the start-up of an application image (checks, code files git;)
        - Main application in master pods or container can t run till init containers have completed successfully; 
 exo: 
        $kubectl api-resources => look for all services  requested by kubectl and their shortnames; use (,) to rquest more services 
        - for statefulsets like in Cassandra, when linking pods w/ their pv, in case a pod terminated => pv still exists (cqfd)
        - est ce que le Cluster (cassandra) est production ready ? 
                -- en Terme DR: le pire scenario: les nodes tombent, pas le pods ? 
                -- check: pods/nodes: $kubectl get pods -o wide
                -- let's cordon a specific node (from having any pod) to it (ne pas choisir le noeud premier qui a le premier pod cassandra-0)
                        $kubectl cordon gke-cluster-1-default-pool-b8987439-zbzw (pour l ex)
                        $kubectl delete pod cassandra-1 (delete le pod associe)
                -- ce qui se passe: le pod est termine, juste apres on cree le meme pod (a cause du replicas) et on le link au node existant et on lui link le pv existant
        - est-ce qu on est pret pour la prod? NON, le cluster est Zonal il faut le rendre regional.....
                -- Need regional availability for HA: Pods in other zone should have their PV in the same zone!!!
                -- Think to a separate Storage provider that can REPLICATE OUR DATA BETWEEN OUR NODES AND ZONES, EXEMPLE: ROOK;
                -- Use rook: create common.yaml, operator.yaml => create ceph which is the backend storage for rook 
                -- when pods are ready: create ceph cluster (yaml): $kubectl create -f ceph-cluster.yaml 
                -- on deploie le statefulset apres: $kubectl apply -f xxx-statefulset.yaml 
                -- $kubectl get pv: voir la storageClass si ca change et dorenavant pris en compte par rook
     ---------------------------------------------------------------------------------------------------------- CI/CD ------------------------------------------------------------------------------------- 
     
     *) GitOps: CI/CD: git, automation system to (push code w/ git, test, build container and deploy to cluster (in different env: branch). 
        tools: Jenkins, but not really cloud native, so GitLab, Gitactions,  google cloud build 
     ---------------------------------------------------------------------------------------------------------- Operators ---------------------------------------------------------------------------------
     *) use: operators extend kubernetes w/ custom resource definition
                - Custom resources: sets of structured  data
                - custom controller: to manage custom resources
     ----------------------------------------------------------------------------------------------------------- Service Mesh (Istio )---------------------------------------------------------------------
     *) centralizes communication and control across all microservices(at scale)
                - Data Plane: 
                        -- a sidecar container running in every pod that manages communication
                        -- a sidecar container running Envoy proxy(istio)
                        -- controls network traffic in and out of the pod
                        -- communicates with Control plane to retreive routing logic and send metrics
                - control plane: deployed set of applications for controlling microservices 
                        -- 3 components in Istio: Pilot: configures data plane, defines proxy rules and behaviors
                        -- mixer: collects traffic metrics and responds to authorization, access control, quota checks 
                        -- citadel: manages identity and security across all services: assigns TLS  certificates, EtE encryption
     *) Istio to install as solution or google cloud Service mesh in anthos services 
        
   ------------------------------------------------------------------------------------------------------------- Serverless on kubernetes -------------------------------------------------------------------
   *) 





























.....

























---
